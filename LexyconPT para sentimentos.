pacman::p_load(devtools, rtweet, tm, RColorBrewer, cluster, fpc, httpuv, SnowballC,
               ggplot2, wordcloud, wordcloud2, tidytext,
               stringr, tidyverse, knitr, png, webshot, htmlwidgets,pkgbuild,lexiconPT, dplyr)
library(lexiconPT)
ls('package:lexiconPT')
ifood = search_tweets("ifood", n = 7500, include_rts = FALSE,lang = "pt")
ifood = ifood[,c('text')]
UberE = search_tweets("uber_eats", n = 7500, include_rts = FALSE,lang = "pt")
UberE = UberE[,c('text')]

Uber_and_Ifood = rbind(UberE,ifood)

# Função para limpeza dos tweets (feita por Estadidados)
f_clean_tweets <- function (tweets) {
  
  clean_tweets <- tweets$text
  clean_tweets = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', ' ', clean_tweets)# remove retweet
  clean_tweets = gsub('@\\w+', ' ', clean_tweets)# remove nomes pessoas
  clean_tweets = gsub('[[:punct:]]', ' ', clean_tweets)# remove pontuaÃ§Ã£o
  clean_tweets = gsub('[[:digit:]]', ' ', clean_tweets)# remove nÃºmeros
  clean_tweets = gsub('http\\w+', ' ', clean_tweets)# remove html links
  clean_tweets = gsub('[ \t]{2,}', ' ', clean_tweets)# remove espaÃ§os desnecessÃ¡rios
  clean_tweets = gsub('^\\s+|\\s+$', ' ', clean_tweets)##
  clean_tweets = gsub('<.*>', ' ', enc2native(clean_tweets))# removecaracteres especiais
  clean_tweets = gsub('\\n', ' ', clean_tweets)# remove quebra de linha
  clean_tweets = gsub('[ \t]{2,}', ' ', clean_tweets)# remove espaÃ§os desnecessÃ¡rios
  clean_tweets = tolower(clean_tweets)# coloca tudo em minÃºsculo
  
  tweets$text <- clean_tweets
  tweets <- tweets[!duplicated(tweets$text),]# remove tweets duplicados
  tweets
}

Uber_and_Ifood = f_clean_tweets(Uber_and_Ifood)

#Função criada para identificar os spams de promoção e oferta e substituí-los por 'NA'.

for(i in 1:(length(Uber_and_Ifood$text))){
  if(stringr::str_sub(Uber_and_Ifood$text[i],start = 0,end = 6) == 'cupons'){
    Uber_and_Ifood$text[i] = '<>'
  }
  if(stringr::str_sub(Uber_and_Ifood$text[i],start = 0,end = 5) == 'cupom'){
    Uber_and_Ifood$text[i] = '<>'  
    
  }
  if(stringr::str_sub(Uber_and_Ifood$text[i],start = 0,end = 6) == 'quer c'){
    Uber_and_Ifood$text[i] = '<>'
  }
  if(Uber_and_Ifood$text[i] == '<>'){
    Uber_and_Ifood$text[i] = NA        
  }
}

#Retirar as colunas que contém valores 'NA'.

Uber_and_Ifood= Uber_and_Ifood[complete.cases(Uber_and_Ifood[,]),]

#Retirar o primeiro '' que a função clean (defeito da mesma)

for(i in 1:(length(Uber_and_Ifood$text))){
  if(stringr::str_sub(Uber_and_Ifood$text[i],start = 0,end = 1) == ' '){
    Uber_and_Ifood$text[i] = str_sub(Uber_and_Ifood$text[i],start = 2)
  }
}

#função p/ achar o primeiro  termo do ifood e separar os data frames.
for(i in 1:length(Uber_and_Ifood$text)){
  if(stringr::str_sub(Uber_and_Ifood$text[i],start = 0,end = 22) == 'me manda um cuponzinho'){
    cont = i
    break
  }
}
# Removendo stopwords  
Uber_and_Ifood$text = removeWords(Uber_and_Ifood$text, stopwords(kind='pt'))

#Dados do Ifood 'Limpos'
ifood_clean = Uber_and_Ifood[cont:length(Uber_and_Ifood$text),]

#Dados do Ubereats 'Limpos'
UberE_clean = Uber_and_Ifood[1:(cont-1),]

#Agora que temos os dados do ubereats e ifood limpos vamos à analise de sentimentos.
# carregar data.base das palavras e respectivos sentimentos(indicados por polaridades).

data("sentiLex_lem_PT02")
sent <- sentiLex_lem_PT02

#base de dados que atribui sentimentos às palavras.
dados_sentiment = sent
head(dados_sentiment,5)

# usar funçao do tidytext para criar uma linha para cada palavra de um comentario
head(ifood_clean,3)
palavras_ifood <- ifood_clean %>% unnest_tokens(ifood_clean, text)
head(palavras_ifood,5)
palavras_UberE <- UberE_clean %>% unnest_tokens(UberE_clean, text)

#Alterar os nomes para, no próximo passo juntar(inner.join) por eles.

names(palavras_ifood)[names(palavras_ifood)== "ifood_clean"]<- "term"
names(palavras_UberE)[names(palavras_UberE)== "UberE_clean"]<- "term"

#Agora temos um data.frame com cada palavra sendo uma linha, assim podendo executar
#o inner.join para analise dos sentimentos.

sentimentos_ifood = palavras_ifood %>% 
  inner_join(dados_sentiment, by = "term")

head(sentimentos_ifood,5)
sentimentos_UberE = palavras_UberE  %>% 
  inner_join(dados_sentiment, by = "term")

# media das polaridades indicando o grau do sentimento (sendo mais proximo de 0 = neutro)!
# mais proximo de 0 = neutro!
# 1 = perfeitamente positivo e -1 = perfeitamente negativo. 
media_ifood = mean(sentimentos_ifood$polarity)
media_UberE = mean(sentimentos_UberE$polarity)

resumo_sentimentos = data.frame(ifood = c('-0.2415795'),
                                UberEats = c('-0.3096515'))
resumo_sentimentos
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

## Sobre Clusters:
Eu peguei esses códigos desse site https://www.tidytextmining.com/sentiment.html no tópico 2.1
install.packages("tidytext")
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)

# Pegando os livros para análises
palavras_sentimentos <- austen_books() %>%
  group_by(book) %>%
  mutate(NumLinha = row_number(),
         capitulo = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
  get_sentiments("nrc")

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

palavras_sentimentos %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

