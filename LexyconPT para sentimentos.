pacman::p_load(devtools, rtweet, tm, RColorBrewer, cluster, fpc, httpuv, SnowballC,
               ggplot2, wordcloud, wordcloud2, tidytext,
               stringr, tidyverse, knitr, png, webshot, htmlwidgets,pkgbuild,lexiconPT)

library(lexiconPT)
ls('package:lexiconPT')

ifood = search_tweets("ifood", n = 10000, include_rts = FALSE,lang = "pt")
ifood = ifood[,c('text')]
UberE = search_tweets("uber_eats", n = 10000, include_rts = FALSE,lang = "pt")
UberE = UberE[,c('text')]

saveIfood = ifood
saveUberE = UberE

Uber_and_Ifood = rbind(UberE,ifood)

# Função para limpeza dos tweets
f_clean_tweets <- function (tweets) {
  
  clean_tweets <- tweets$text
  # remove retweet 
  clean_tweets = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', ' ', clean_tweets)
  # remove nomes pessoas
  clean_tweets = gsub('@\\w+', ' ', clean_tweets)
  # remove pontuaÃ§Ã£o
  clean_tweets = gsub('[[:punct:]]', ' ', clean_tweets)
  # remove nÃºmeros
  clean_tweets = gsub('[[:digit:]]', ' ', clean_tweets)
  # remove html links
  clean_tweets = gsub('http\\w+', ' ', clean_tweets)
  # remove espaÃ§os desnecessÃ¡rios
  clean_tweets = gsub('[ \t]{2,}', ' ', clean_tweets)
  clean_tweets = gsub('^\\s+|\\s+$', ' ', clean_tweets)
  # remove emojis e caracteres especiais
  clean_tweets = gsub('<.*>', ' ', enc2native(clean_tweets))
  # remove quebra de linha
  clean_tweets = gsub('\\n', ' ', clean_tweets)
  # remove espaÃ§os desnecessÃ¡rios
  clean_tweets = gsub('[ \t]{2,}', ' ', clean_tweets)
  # coloca tudo em minÃºsculo
  clean_tweets = tolower(clean_tweets)
  
  # remove tweets duplicados
  tweets$text <- clean_tweets
  tweets <- tweets[!duplicated(tweets$text),]
  tweets
}

Uber_and_Ifood = f_clean_tweets(Uber_and_Ifood)

#Função criada para identificar os spams de promoção e oferta e substorui-los por 'NA'.

for(i in 1:(length(Uber_and_Ifood$text))){
  if(stringr::str_sub(Uber_and_Ifood$text[i],start = 0,end = 6) == 'cupons'){
    Uber_and_Ifood$text[i] = '<>'
  }
  if(stringr::str_sub(Uber_and_Ifood$text[i],start = 0,end = 5) == 'cupom'){
    Uber_and_Ifood$text[i] = '<>'  
    
  }
  if(stringr::str_sub(Uber_and_Ifood$text[i],start = 0,end = 6) == 'quer c'){
    Uber_and_Ifood$text[i] = '<>'
  }
  if(Uber_and_Ifood$text[i] == '<>'){
    Uber_and_Ifood$text[i] = NA        
  }
}

#Retirar as colunas que contém valores 'NA'.
Uber_and_Ifood= Uber_and_Ifood[complete.cases(Uber_and_Ifood[,]),]

#função p/ achar o primeiro  termo do ifood e separar os data frames.
for(i in 1:8620){
  if(stringr::str_sub(Uber_and_Ifood$text[i],start = 0,end = 17) == 'pela primeira vez'){
    cont = i
  }
}
# Removendo stopwords  
Uber_and_Ifood$text = removeWords(Uber_and_Ifood$text, stopwords(kind='pt'))

#Dados do Ifood 'Limpos'
ifood_clean = Uber_and_Ifood[5805:14814,]

#Dados do Ubereats 'Limpos'
UberE_clean = Uber_and_Ifood[1:5804,]

#Agora que temos os dados do ubereats e ifood limpos vamos à analise de sentimentos.

# carregar datasets
data("oplexicon_v3.0")
data("sentiLex_lem_PT02")

op30 <- oplexicon_v3.0
sent <- sentiLex_lem_PT02 

dados_sentiment = sent[,c('term','polarity')]

names(ifood_clean)[names(ifood_clean)== "text"]<- "term"  #Em Desenvolvimento!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

## Sobre Clusters:
Eu peguei esses códigos desse site https://www.tidytextmining.com/sentiment.html no tópico 2.1
install.packages("tidytext")
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)

# Pegando os livros para análises
palavras_sentimentos <- austen_books() %>%
  group_by(book) %>%
  mutate(NumLinha = row_number(),
         capitulo = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
  get_sentiments("nrc")

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

palavras_sentimentos %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

